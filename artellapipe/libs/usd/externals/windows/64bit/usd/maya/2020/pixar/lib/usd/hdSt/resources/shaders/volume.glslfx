-- glslfx version 0.1

//
// Copyright 2019 Pixar
//
// Licensed under the Apache License, Version 2.0 (the "Apache License")
// with the following modification; you may not use this file except in
// compliance with the Apache License and the following modification to it:
// Section 6. Trademarks. is deleted and replaced with:
//
// 6. Trademarks. This License does not grant permission to use the trade
//    names, trademarks, service marks, or product names of the Licensor
//    and its affiliates, except as required to comply with Section 4(c) of
//    the License and to reproduce the content of the NOTICE file.
//
// You may obtain a copy of the Apache License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the Apache License with the above modification is
// distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied. See the Apache License for the specific
// language governing permissions and limitations under the Apache License.
//

--- This is what an import might look like.
--- #import $TOOLS/hdSt/shaders/volume.glslfx

#import $TOOLS/hdSt/shaders/instancing.glslfx
#import $TOOLS/hdSt/shaders/pointId.glslfx

--- --------------------------------------------------------------------------
-- glsl Volume.Vertex

out VertexData
{
    // Relying on perspectively correct interpolation.
    vec3 Peye;
} outData;

void main(void)
{
    // Bounding box vertex in local spce
    const vec4 point = vec4(HdGet_points().xyz, 1);

    const MAT4 transform  = ApplyInstanceTransform(HdGet_transform());

    // Bounding box vertex in eye space.
    const vec4 pointEye = vec4(GetWorldToViewMatrix() * transform * point); 

    outData.Peye = pointEye.xyz / pointEye.w;

    gl_Position = vec4(GetProjectionMatrix() * pointEye);

    ProcessPrimvars();
}

--- --------------------------------------------------------------------------
-- glsl Volume.Fragment

// Quality knobs, should eventually be configurable.
//
// We also might have different values for the raymarch
// integrating the pixel value and for the raymarch doing
// the lighting computation.

const int maxNumSteps = 10000;

in VertexData
{
    vec3 Peye;
} inData;

// Transform a point by a 4x4 matrix
vec3
transformPoint(mat4 m, vec3 point)
{
    const vec4 result = m * vec4(point, 1.0);
    return result.xyz / result.w;
}

// Transform a point by a 4x4 matrix
vec3
transformPoint(dmat4 m, vec3 point)
{
    const vec4 result = vec4(m * vec4(point, 1.0));
    return result.xyz / result.w;
}

// Is point (in local coordinates) in bounding box.
bool
inBoundingBox(vec3 p)
{
    const vec3 pVol = transformPoint(HdGet_volumeBBoxInverseTransform(), p);

    // Bounding box
    const vec3 localMin = vec3(HdGet_volumeBBoxLocalMin().xyz);
    const vec3 localMax = vec3(HdGet_volumeBBoxLocalMax().xyz);

    return
        all(lessThanEqual(localMin, pVol)) &&
        all(lessThanEqual(pVol, localMax));
}

// Matrix to go from eye space to local space.
// Used frequently per ray-marching step in both volumeIntegrator
// and accumulatedTransmittance, so computed only once in main.
//
MAT4 instanceModelViewInverse;

vec3
eyeToLocal(vec3 p)
{
    return transformPoint(instanceModelViewInverse, p);
}

#if NUM_LIGHTS == 0

vec3
lightingComputation(vec3 rayPointEye, vec3 rayDirectionEye)
{
    return vec3(0.0);
}

#else

// Compute how the transmittance of volume from Peye to a
// light source in the given direction rayDirection.
// This integrates the density from Peye to the boundary of
// the volume. The assumption is that the light source is
// out of the volume.
float
accumulatedTransmittance(vec3 rayStartEye, vec3 rayDirectionEye)
{
    const float stepSize = HdGet_stepSizeLighting();
    int i = 1;
    
    float totalExtinction = 0.0;
    
    while(i < maxNumSteps) {
        const vec3 rayPointEye = rayStartEye + stepSize * i * rayDirectionEye;
        const vec3 rayPoint = eyeToLocal(rayPointEye);

        if (!inBoundingBox(rayPoint)) {
            break;
        }

        totalExtinction += extinctionFunction(rayPoint);

        i+=1;
    }

    return exp(-totalExtinction * stepSize);
}

// Computes amount of light arriving at point Peye
// taking attenuation (e.g., by inverse-square law), shadows,
// transmittance by volume into account.
vec3
lightingComputation(vec3 rayPointEye, vec3 rayDirectionEye)
{
    vec3 result = vec3(0.0);
    for (int i = 0; i < NUM_LIGHTS; ++i) {

        const vec4 Plight = lightSource[i].position;

        const vec3 lightDirectionEye = normalize(
            (Plight.w == 0.0) ? Plight.xyz : Plight.xyz - rayPointEye);

        const float atten =
            lightDistanceAttenuation(vec4(rayPointEye,1), i) *
            lightSpotAttenuation(lightDirectionEye, i);

// For now, not using shadows for volumes.
#if USE_SHADOWS && 0
        const float shadow = (lightSource[i].hasShadow) ?
            shadowing(lightSource[i].shadowIndex, rayPointEye) : 1.0;
#else
        const float shadow = 1.0;
#endif

        if (shadow > 0.0001) {
            result +=
                shadow *
                atten *
                // Assuming that light source is outside of volume's
                // bounding box (might integrate extinction along ray
                // beyond light source).
                accumulatedTransmittance(rayPointEye, lightDirectionEye) *
                phaseFunction(-rayDirectionEye, lightDirectionEye) *
                lightSource[i].diffuse.rgb;
        }
    }

    return result;
}

#endif

// Result of integrating volume along a ray
struct VolumeContribution
{
    // Coordinates where ray marching hit the first non-empty voxel
    // in eye space. 0 indicates the ray hit only empty voxels.
    vec3 firstHitPeye;

    // Integrated color
    vec3 color;

    // Integrated transmittance, i.e., what fraction of light from
    // geometry behind the volume is still visible.
    float transmittance;
};

VolumeContribution
volumeIntegrator(vec3 rayStartEye, vec3 rayDirectionEye, float rayEndDepth)
{
    const float stepSize = HdGet_stepSize();
    int i = 1;

    VolumeContribution result;
    result.firstHitPeye = vec3(0.0);
    result.color = vec3(0.0);
    result.transmittance = 1.0;

    // integrate transmittance and light along ray for bounding box
    while(i < maxNumSteps) {
        const vec3 rayPointEye = rayStartEye + stepSize * i * rayDirectionEye;
        const vec3 rayPoint = eyeToLocal(rayPointEye);

        // Stop when outside of volume bounding box
        if (!inBoundingBox(rayPoint)) {
            break;
        }

        // Stop when hitting opaque geometry in the volume.
        if (rayPointEye.z < rayEndDepth) {
            break;
        }

        // Evaluate volume shader functions to determine extinction,
        // scattering, and emission.
        const float extinctionValue = extinctionFunction(rayPoint);
        const float scatteringValue = scatteringFunction(rayPoint);
        const vec3 emissionValue = emissionFunction(rayPoint);

        // If this is the first time the ray is hitting a non-empty voxel,
        // record the coordinates.
        if (result.firstHitPeye == vec3(0.0)) {
            if (  extinctionValue > 0 ||
                  scatteringValue > 0 ||
                  any(greaterThan(emissionValue, vec3(0)))) {
                result.firstHitPeye = rayPointEye;
            }
        }

        // In scattering contribution
        const vec3 inScattering =
            scatteringValue *
            lightingComputation(rayPointEye, rayDirectionEye);

        // In scattering and emission contribution
        result.color += 
            (stepSize * result.transmittance) *
            (inScattering + emissionValue);

        // Update transmittance
        result.transmittance *= exp(-extinctionValue * stepSize);

        i+=1;
    }

    return result;
}

// Is camera orthographic?
bool
isCameraOrthographic()
{
    return abs(GetProjectionMatrix()[3][3] - 1.0) < 1e-5;
}

// Convert depth value z in [-1,1] to depth in eye space [-near, -far].
float
NDCtoEyeDepth(float z)
{
    const MAT4 m = inverse(GetProjectionMatrix());
    return (m[2][2] * z + m[3][2]) / (m[2][3] * z + m[3][3]);
}

// Compute the near clipping distance. Always returns a positive value.
float
computeNearDistance()
{
    return abs(NDCtoEyeDepth(-1.0));
}

// Given an eye ray (ray from eye to given point in eye space),
// computes the intersection of the ray with near clipping plane.
vec3
computeNearPointEye(vec3 Peye)
{
    // Transform point from eye to clip space.
    const vec4 Pclip = GetProjectionMatrix() * vec4(Peye, 1.0);
    // Project point onto near clipping plane in clip space
    // (equivalently, set z to -1 in NDC).
    const vec4 nearPointClip = vec4(Pclip.xy, -Pclip.w, Pclip.w);
    // Convert back to eye space
    const vec4 nearPointEye = inverse(GetProjectionMatrix()) * nearPointClip;

    return nearPointEye.xyz / nearPointEye.w;
}

// Compute the point from which we start ray marching. The input is
// the point on the boundary of the volume bounding box corresponding to this
// fragment. Input and output are both in eye space.
//
// If the camera (or more precisely, the point where the eye ray
// intersects the near clipping plane) is inside the volume, then the
// fragment shader is called only for the point where the eye ray is
// exiting the bounding box. To obtain the correct result in this
// case, we need to move the start point close to the camera.
//
vec3
computeRayStartEye(vec3 Peye)
{
    // Compute where eye ray intersects near clipping plane in eye space ...
    const vec3 nearClippingPointEye = computeNearPointEye(Peye);
    // ... and local space.
    const vec3 nearClippingPoint = eyeToLocal(nearClippingPointEye);

    // If that point is inside the bounding box of the volume, ...
    if (inBoundingBox(nearClippingPoint)) {
        // ... we need to move the point close to the camera.
        if (isCameraOrthographic()) {
            // If orthographic, take intersectin of eye ray with
            // near clipping plane.
            return nearClippingPointEye;
        } else {
            //
            // This point could be obtained by either intersecting the
            // eye ray with the near clipping plane or with a sphere
            // about there eye (with radius being the near distance).
            //
            // Note that in the former case, the distance between the
            // eye and the point where raymarching starts is
            // non-constant across the image. This could mean that we
            // skip more volume away the center of the image making the
            // image darker there - so we see opposite vignetting.
            // To avoid this bias, we use the latter.
            //
            // Note that we can use points in front of the near plane
            // since OIT resolution makes no assumptions about the
            // depth value.
            //
            return normalize(Peye) * computeNearDistance();
        }
    }
    
    // Otherwise, just return input.
    return Peye;
}

// Consider the ray from the eye to a given point in eye space.
// Computes the direction of this ray in both cases where the
// camera is orthographic or perspective.
vec3
computeRayDirectionEye(vec3 rayStartEye)
{
    // In NDC space, the ray is always pointing into the z-direction (0,0,1).
    // In clip space, this corresponds to (0,0,1,0).
    // We need to multiply (0,0,1,0) by the inverse projection matrix to
    // get to homogeneous eye space.
    // Or alternatively, we can get the direction in homogeneous eye space
    // by taking the respective column of the inverse projection matrix:
    const vec4 dir = inverse(GetProjectionMatrix())[2];

    // To compute the corresponding direction in non-homogeneous eye space,
    // compute the position of the ray after time dt << 1:
    //     vec4 pHomogeneous = vec4(rayStartEye, 1.0) + dt * dir;
    //     vec3 p = pHomogeneous.xyz / pHomogeneous.w;
    //
    // Or equivalently:
    //     vec3 p = (rayStartEye + dt * dir.xyz) / (1.0 + dir.w * dt);
    // And since dt << 1, we have
    //     vec3 p = (rayStartEye + dt * dir.xyz) * (1.0 - dir.w * dt);
    // And dropping higher order terms:
    //     vec3 p = rayStartEye + dt * (dir.xyz - rayStartEye * dir.w);
    // So the new direction is given by:
    //     vec3 d = dir.xyz - rayStartEye * dir.w;

    // Normalize direction in eye space.
    return normalize(dir.xyz - rayStartEye * dir.w);
}

// The depth at which we hit opaque geometry in eye space (negative
// value by OpenGL convention).
float
computeRayEndDepth(vec2 fragcoord)
{
#ifdef HD_HAS_depthReadback
    // Sample the z-Buffer at the frag coordinate.
    const float bufferVal = texelFetch(HdGetSampler_depthReadback(),
                                       ivec2(fragcoord),
                                       /* lod = */ 0).z;
#else
    // Assume far-plane if we cannot sample the z-Buffer.
    const float bufferVal = 1.0;
#endif

    return NDCtoEyeDepth(2.0 * bufferVal - 1.0);
};

void main(void)
{
    instanceModelViewInverse =
        ApplyInstanceTransformInverse(HdGet_transformInverse()) *
        GetWorldToViewInverseMatrix();
    
    // camera facing.
    const vec3 Neye = vec3(0, 0, 1);

    const vec3 rayStartEye = computeRayStartEye(inData.Peye);
    const vec3 rayDirectionEye = computeRayDirectionEye(rayStartEye);

    // The depth (z-value) at which we stop raymarching because we hit
    // opaque geometry.
    // This will be a negative value since the camera is looking down
    // the minus z-direction by OpenGL conventions.
    const float rayEndDepth = computeRayEndDepth(gl_FragCoord.xy);

    const VolumeContribution volumeContribution =
        volumeIntegrator(rayStartEye, rayDirectionEye, rayEndDepth);
    const float alpha = 1 - volumeContribution.transmittance;
    const vec4 color = ApplyColorOverrides(vec4(volumeContribution.color, alpha));

    const vec4 patchCoord = vec4(0.0);

    RenderOutput(vec4(volumeContribution.firstHitPeye, 1),
                 Neye, color, patchCoord);
}
